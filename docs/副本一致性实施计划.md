# 副本一致性实施计划

## 1. 概述

本文档描述CrabMQ副本一致性功能的详细实现方案，基于Leader-Follower架构，参考Kafka的副本机制设计。

### 1.1 目标
- 实现Partition的副本复制机制，保证数据高可用
- 支持Leader自动选举和故障转移
- 保证数据一致性（HW/LEO机制）
- 支持动态扩容和缩容

### 1.2 术语说明
- **fp (follower partition)**: Follower Partition，从副本
- **lp (leader partition)**: Leader Partition，主副本
- **cg (consumer group)**: Consumer Group，消费者组
- **cgo (consumer group offset)**: Consumer Group Offset，消费者组偏移量
- **coo (coordinator)**: Coordinator，协调者节点
- **b (broker)**: Broker，消息代理节点
- **pc (partition_cluster)**: Partition Cluster，分区集群
- **p (partition)**: Partition，分区
- **hw (high water mark)**: High Water Mark，高水位标记
- **leo (log end offset)**: Log End Offset，日志末端偏移量

---

## 2. 核心概念

### 2.1 Partition集群
- **定义**: 同一Topic的同一Partition在不同Broker上的所有副本组成一个Partition集群
- **成员**: 1个Leader Partition + N个Follower Partitions
- **形成时机**: 在coo创建Topic时自动形成，确定哪些Broker上有该Partition的副本

### 2.2 ID生成规则
- **partition_id**: 在coo创建partition时生成，Topic内唯一，从0开始递增
- **partition_cluster_id**: 在coo创建partition时生成，通过`hash(topic_name, partition_id)`生成，全局唯一
- **副本标识**: 通过`(broker_id, partition_cluster_id)`唯一标识一个Partition副本

### 2.3 角色职责
- **Leader Partition**: 
  - 处理所有读写请求
  - 记录全局HW和自身LEO
  - 记录每个Follower的LEO
  - 计算并更新全局HW
  
- **Follower Partition**:
  - 仅记录全局HW和自身LEO
  - 定期从Leader拉取数据
  - 不直接服务客户端请求

---

## 3. 数据结构设计

### 3.1 Coordinator端数据结构

```rust
// Coordinator中存放的partition及其集群信息
struct CooPartitionInfo {
    // 主索引：broker_id => BrokerPartitionInfo
    brokers: DashMap<u32, BrokerPartitionInfo>,
    
    // 主索引：partition_cluster_id => PartitionClusterInfo
    partition_cluster: DashMap<u32, PartitionClusterInfo>,

    // 主索引：partition_id => PartitionInfo
    partitions: DashMap<u32, PartitionInfo>,
    
    // 辅助索引1：快速通过 (broker_id, partition_cluster_id) 查找 partition
    // 格式：复合键 "broker_id:partition_cluster_id" => Vec<partition_id>
    broker_cluster_index: DashMap<String, Vec<u32>>,
    
    // 辅助索引2：快速通过 broker_id 查找该 broker 下的所有 partition_id
    broker_partition_index: DashMap<u32, Vec<u32>>,
    
    // 辅助索引3：快速通过 partition_cluster_id 查找该集群下的所有 partition_id
    cluster_partition_index: DashMap<u32, Vec<u32>>,
}

impl CooPartitionInfo {
    // 通过 broker_id 和 partition_cluster_id 快速检索 partition 信息
    fn get_partitions_by_broker_cluster(&self, broker_id: u32, cluster_id: u32) -> Vec<PartitionInfo> {
        let key = format!("{}:{}", broker_id, cluster_id);
        if let Some(partition_ids) = self.broker_cluster_index.get(&key) {
            partition_ids.iter()
                .filter_map(|pid| self.partitions.get(pid).map(|p| p.clone()))
                .collect()
        } else {
            Vec::new()
        }
    }
    
    // 通过 broker_id 快速检索该 broker 下的所有 partition
    fn get_partitions_by_broker(&self, broker_id: u32) -> Vec<PartitionInfo> {
        if let Some(partition_ids) = self.broker_partition_index.get(&broker_id) {
            partition_ids.iter()
                .filter_map(|pid| self.partitions.get(pid).map(|p| p.clone()))
                .collect()
        } else {
            Vec::new()
        }
    }
    
    // 通过 partition_cluster_id 快速检索该集群下的所有 partition
    fn get_partitions_by_cluster(&self, cluster_id: u32) -> Vec<PartitionInfo> {
        if let Some(partition_ids) = self.cluster_partition_index.get(&cluster_id) {
            partition_ids.iter()
                .filter_map(|pid| self.partitions.get(pid).map(|p| p.clone()))
                .collect()
        } else {
            Vec::new()
        }
    }
    
    // 更新 partition 信息时，同步更新所有索引
    fn update_partition(&self, partition: PartitionInfo) {
        let partition_id = partition.id;
        let broker_id = partition.broker_id;
        let cluster_id = partition.partition_cluster_id;
        
        // 更新主索引
        self.partitions.insert(partition_id, partition.clone());
        
        // 更新 broker_cluster_index
        let broker_cluster_key = format!("{}:{}", broker_id, cluster_id);
        self.broker_cluster_index
            .entry(broker_cluster_key)
            .or_insert_with(Vec::new)
            .push(partition_id);
        
        // 更新 broker_partition_index
        self.broker_partition_index
            .entry(broker_id)
            .or_insert_with(Vec::new)
            .push(partition_id);
        
        // 更新 cluster_partition_index
        self.cluster_partition_index
            .entry(cluster_id)
            .or_insert_with(Vec::new)
            .push(partition_id);
    }
}

struct BrokerPartitionInfo {
    // partition_cluster_id => PartitionClusterInfo
    clusters: DashMap<u32, PartitionClusterInfo>,
    
    // partition_id => PartitionInfo
    partitions: DashMap<u32, PartitionInfo>,
    
    // broker 最后更新时间（用于检测超时）
    latest_update_time: u64,
}

struct PartitionClusterInfo {
    cluster_id: u32,
    lp_id: u32,  // Leader partition id
    fp_ids: Vec<u32>,  // Follower partition ids
    hw: SegmentOffset,  // high water mark
    cgo: DashMap<u64, SegmentOffset>,  // consumer group offset
    epoch: u64,  // Leader epoch，每次选举时递增，用于防止过期 Leader
}

struct PartitionInfo {
    id: u32,
    partition_cluster_id: u32,
    is_leader: bool,
    broker_id: u32,
    leo: u64,  // Log End Offset
    hw: u64,   // High Water Mark
    epoch: u64, // Leader epoch，与 PartitionClusterInfo.epoch 保持一致
}
```

### 3.2 Broker端数据结构

```rust
// Broker中存放的partition及其集群信息
struct BrokerPartitionInfo {
    // partition_cluster_id => PartitionClusterInfo
    clusters: DashMap<u32, PartitionClusterInfo>,
    
    // partition_id => PartitionInfo
    partitions: DashMap<u32, PartitionInfo>,
    
    // 辅助索引：快速通过 partition_cluster_id 查找该集群下的所有 partition_id
    cluster_partition_index: DashMap<u32, Vec<u32>>,
}

impl BrokerPartitionInfo {
    // 通过 partition_cluster_id 快速检索该集群下的所有 partition
    fn get_partitions_by_cluster(&self, cluster_id: u32) -> Vec<PartitionInfo> {
        if let Some(partition_ids) = self.cluster_partition_index.get(&cluster_id) {
            partition_ids.iter()
                .filter_map(|pid| self.partitions.get(pid).map(|p| p.clone()))
                .collect()
        } else {
            Vec::new()
        }
    }
    
    // 更新 partition 信息时，同步更新索引
    fn update_partition(&self, partition: PartitionInfo) {
        let partition_id = partition.id;
        let cluster_id = partition.partition_cluster_id;
        
        // 更新主索引
        self.partitions.insert(partition_id, partition.clone());
        
        // 更新 cluster_partition_index
        self.cluster_partition_index
            .entry(cluster_id)
            .or_insert_with(Vec::new)
            .push(partition_id);
    }
}
```

---

## 4. 实现步骤

### 4.1 阶段1：基础数据结构实现

**目标**: 实现数据结构和索引管理

**任务清单**:
1. [ ] 实现 `CooPartitionInfo` 结构体及其方法
2. [ ] 实现 `BrokerPartitionInfo` 结构体及其方法
3. [ ] 实现 `PartitionClusterInfo` 结构体
4. [ ] 实现 `PartitionInfo` 结构体
5. [ ] 实现索引的增删改查操作
6. [ ] 编写单元测试验证索引功能

**验收标准**:
- 能够通过broker_id快速查找partition
- 能够通过partition_cluster_id快速查找partition
- 能够通过(broker_id, partition_cluster_id)快速查找partition
- 索引更新操作线程安全

### 4.2 阶段2：Topic创建和Partition分配

**目标**: 实现Topic创建时的Partition分配逻辑

**实现流程**:
```rust
async fn create_topic_partitions(
    coordinator: &Coordinator,
    topic: &str,
    num_partitions: u32,
    replication_factor: usize,
) -> Result<Vec<PartitionInfo>> {
    // 1. 选择可用的Broker（根据分配策略：轮询/负载感知等）
    let available_brokers = coordinator.select_available_brokers(replication_factor);
    
    // 2. 为每个Partition生成ID和Cluster ID
    let mut partitions = Vec::new();
    for partition_index in 0..num_partitions {
        // 2.1 生成Partition ID（Topic内唯一，从0开始递增）
        let partition_id = coordinator.generate_partition_id(topic);
        
        // 2.2 生成Cluster ID（全局唯一，基于Topic和Partition ID）
        let cluster_id = generate_cluster_id(topic, partition_id);
        
        // 2.3 选择Leader和Followers
        let (leader_broker_id, follower_broker_ids) = 
            coordinator.select_replicas(&available_brokers, partition_index as usize);
        
        // 2.4 创建Partition集群信息
        let cluster_info = PartitionClusterInfo {
            cluster_id,
            lp_id: partition_id,  // 初始Leader ID
            fp_ids: Vec::new(),  // 初始为空，待创建Follower后更新
            hw: SegmentOffset::default(),
            cgo: DashMap::new(),
            epoch: 0,
        };
        coordinator.partition_cluster.insert(cluster_id, cluster_info);
        
        // 2.5 创建Leader Partition
        let leader_partition = PartitionInfo {
            id: partition_id,
            partition_cluster_id: cluster_id,
            is_leader: true,
            broker_id: leader_broker_id,
            leo: 0,
            hw: 0,
            epoch: 0,
        };
        coordinator.update_partition(leader_partition.clone());
        partitions.push(leader_partition);
        
        // 2.6 创建Follower Partitions
        let mut follower_partition_ids = Vec::new();
        for follower_broker_id in follower_broker_ids {
            let follower_partition = PartitionInfo {
                id: partition_id,  // 注意：Follower和Leader使用相同的partition_id
                partition_cluster_id: cluster_id,
                is_leader: false,
                broker_id: follower_broker_id,
                leo: 0,
                hw: 0,
                epoch: 0,
            };
            coordinator.update_partition(follower_partition.clone());
            follower_partition_ids.push(partition_id);
            partitions.push(follower_partition);
        }
        
        // 2.7 更新Cluster Info中的Follower IDs
        let mut cluster_info = coordinator.partition_cluster.get(&cluster_id).unwrap();
        cluster_info.fp_ids = follower_partition_ids;
        coordinator.partition_cluster.insert(cluster_id, cluster_info);
    }
    
    Ok(partitions)
}

// Cluster ID生成函数
fn generate_cluster_id(topic: &str, partition_id: u32) -> u32 {
    use std::collections::hash_map::DefaultHasher;
    use std::hash::{Hash, Hasher};
    
    let mut hasher = DefaultHasher::new();
    topic.hash(&mut hasher);
    partition_id.hash(&mut hasher);
    
    // 取低32位作为cluster_id
    (hasher.finish() & 0xFFFFFFFF) as u32
}
```

**任务清单**:
1. [ ] 实现 `generate_cluster_id` 函数
2. [ ] 实现 `generate_partition_id` 函数（支持持久化）
3. [ ] 实现 `select_available_brokers` 函数（支持多种策略）
4. [ ] 实现 `select_replicas` 函数（选择Leader和Followers）
5. [ ] 实现 `create_topic_partitions` 函数
6. [ ] 集成到Topic创建流程中
7. [ ] 编写单元测试和集成测试

**验收标准**:
- Topic创建时正确分配Partition到不同Broker
- Partition ID在Topic内唯一且递增
- Cluster ID全局唯一且可预测
- 副本分布符合分配策略

### 4.3 阶段3：心跳机制和元数据同步

**目标**: 实现Broker与Coordinator之间的心跳机制

**实现流程**:
```rust
// Broker端：定期发送心跳
async fn broker_heartbeat_loop(broker: &Broker) {
    let mut interval = interval(Duration::from_secs(HEARTBEAT_INTERVAL));
    
    loop {
        interval.tick().await;
        
        // 1. 收集本地Partition信息
        let partition_states: Vec<PartitionState> = broker
            .partitions
            .iter()
            .map(|p| PartitionState {
                partition_id: p.id,
                partition_cluster_id: p.partition_cluster_id,
                is_leader: p.is_leader,
                leo: p.leo,
                hw: p.hw,
                epoch: p.epoch,
            })
            .collect();
        
        // 2. 发送心跳到Coordinator
        let heartbeat_req = BrokerHeartbeatReq {
            broker_id: broker.id,
            partition_states,
            timestamp: current_timestamp(),
        };
        
        match coordinator.heartbeat(heartbeat_req).await {
            Ok(resp) => {
                // 3. 处理Coordinator返回的角色变更信息
                for role_change in resp.role_changes {
                    broker.handle_role_change(role_change).await?;
                }
                
                // 4. 更新本地Partition信息
                broker.update_partition_info(resp.partition_info).await?;
            }
            Err(e) => {
                error!("Heartbeat failed: {}", e);
                // 心跳失败处理：租约过期后停止写入
                broker.handle_heartbeat_failure().await?;
            }
        }
    }
}

// Coordinator端：处理心跳并更新元数据
async fn coordinator_handle_heartbeat(
    &self,
    req: BrokerHeartbeatReq,
) -> Result<BrokerHeartbeatResp> {
    let broker_id = req.broker_id;
    
    // 1. 更新Broker的最后更新时间
    if let Some(broker_info) = self.partition_info.brokers.get_mut(&broker_id) {
        broker_info.latest_update_time = current_timestamp();
    }
    
    // 2. 更新Partition信息（LEO、HW等）
    for state in req.partition_states {
        if let Some(partition) = self.partition_info.partitions.get_mut(&state.partition_id) {
            partition.leo = state.leo;
            partition.hw = state.hw;
            partition.epoch = state.epoch;
        }
    }
    
    // 3. 检查是否有角色变更需要通知
    let role_changes = self.check_role_changes(broker_id).await?;
    
    // 4. 返回该Broker的Partition信息
    let partition_info = self.partition_info.get_partitions_by_broker(broker_id);
    
    Ok(BrokerHeartbeatResp {
        role_changes,
        partition_info,
    })
}
```

**任务清单**:
1. [ ] 实现Broker端心跳发送逻辑
2. [ ] 实现Coordinator端心跳处理逻辑
3. [ ] 实现元数据更新机制
4. [ ] 实现角色变更通知机制
5. [ ] 实现心跳超时检测
6. [ ] 编写单元测试和集成测试

**验收标准**:
- Broker能够定期发送心跳
- Coordinator能够正确处理心跳并更新元数据
- 心跳超时能够被正确检测
- 角色变更能够及时通知到Broker

### 4.4 阶段4：数据同步机制

**目标**: 实现Follower从Leader拉取数据的机制

**实现流程**:
```rust
// Follower端：定期从Leader拉取数据
async fn follower_sync_loop(follower: &FollowerPartition) {
    let mut interval = interval(Duration::from_secs(SYNC_INTERVAL));
    
    loop {
        interval.tick().await;
        
        // 1. 获取Leader地址
        let leader_addr = get_leader_address(follower.partition_cluster_id).await?;
        
        // 2. 建立或复用TCP连接（支持多路复用）
        let connection = get_or_create_replication_connection(leader_addr).await?;
        
        // 3. 发送拉取请求（包含当前LEO）
        let fetch_req = FetchRequest {
            partition_id: follower.id,
            partition_cluster_id: follower.partition_cluster_id,
            fetch_offset: follower.leo,  // 从当前LEO开始拉取
            max_bytes: MAX_FETCH_BYTES,
        };
        
        match connection.fetch(fetch_req).await {
            Ok(fetch_resp) => {
                // 4. 先更新全局HW（从Leader返回）
                follower.hw = fetch_resp.hw;
                
                // 5. 落盘数据
                if !fetch_resp.messages.is_empty() {
                    follower.write_to_disk(fetch_resp.messages).await?;
                    
                    // 6. 落盘成功后才更新本地LEO
                    follower.leo = fetch_resp.next_offset;
                }
            }
            Err(e) => {
                error!("Fetch failed: {}", e);
                // 重试逻辑
            }
        }
    }
}

// Leader端：处理Follower的拉取请求
async fn leader_handle_fetch(
    &self,
    req: FetchRequest,
) -> Result<FetchResponse> {
    let partition = self.get_partition(req.partition_id)?;
    
    // 1. 读取数据（从req.fetch_offset开始）
    let messages = partition.read_messages(req.fetch_offset, req.max_bytes).await?;
    
    // 2. 更新Follower的LEO（用于计算HW）
    partition.update_follower_leo(req.partition_id, req.fetch_offset + messages.len() as u64);
    
    // 3. 计算全局HW（min(所有ISR的LEO)）
    let hw = partition.calculate_hw();
    partition.hw = hw;
    
    // 4. 返回数据和HW
    Ok(FetchResponse {
        messages,
        hw,
        next_offset: req.fetch_offset + messages.len() as u64,
    })
}

// Leader端：计算HW
fn calculate_hw(&self) -> u64 {
    // 获取所有ISR（In-Sync Replicas）的LEO
    let mut leos = vec![self.leo];  // Leader的LEO
    
    for follower_id in &self.follower_ids {
        if let Some(follower_leo) = self.follower_leos.get(follower_id) {
            leos.push(*follower_leo);
        }
    }
    
    // HW = min(所有ISR的LEO)
    leos.into_iter().min().unwrap_or(0)
}
```

**连接多路复用设计**:
```rust
// Broker级别维护连接池
struct BrokerConnectionPool {
    // target_broker_id => Connection
    connections: DashMap<u32, Arc<ReplicationConnection>>,
}

struct ReplicationConnection {
    stream_id_generator: AtomicU32,
    // stream_id => PartitionReplicationStream
    streams: DashMap<u32, PartitionReplicationStream>,
}

// 同一Broker对之间的所有Partition共享一个TCP连接
// 连接内使用stream_id区分不同Partition的数据流
```

**任务清单**:
1. [ ] 实现Follower拉取数据逻辑
2. [ ] 实现Leader处理拉取请求逻辑
3. [ ] 实现HW计算逻辑
4. [ ] 实现TCP连接多路复用
5. [ ] 实现数据落盘逻辑
6. [ ] 实现错误处理和重试机制
7. [ ] 编写单元测试和集成测试

**验收标准**:
- Follower能够定期从Leader拉取数据
- Leader能够正确计算和更新HW
- 数据落盘成功后才更新LEO
- TCP连接能够多路复用，减少连接数
- 网络异常时能够正确处理和重试

### 4.5 阶段5：Leader选举机制

**目标**: 实现Broker故障时的自动Leader选举

**实现流程**:

**步骤1：Broker失联检测**
```rust
async fn broker_timeout_check_loop(coordinator: &Coordinator) {
    let mut interval = interval(Duration::from_secs(TIMEOUT_CHECK_INTERVAL));
    
    loop {
        interval.tick().await;
        
        let current_time = current_timestamp();
        let timeout_threshold = HEARTBEAT_TIMEOUT_THRESHOLD;
        
        // 遍历所有broker，找出超时的broker
        let timeout_brokers: Vec<u32> = coordinator.partition_info.brokers.iter()
            .filter(|entry| {
                let broker_info = entry.value();
                let elapsed = current_time - broker_info.latest_update_time;
                elapsed > timeout_threshold
            })
            .map(|entry| *entry.key())
            .collect();
        
        // 对每个超时的broker触发选举流程
        for broker_id in timeout_brokers {
            coordinator.handle_broker_timeout(broker_id).await?;
        }
    }
}
```

**步骤2：识别受影响的Partition集群**
```rust
async fn handle_broker_timeout(
    &self,
    failed_broker_id: u32,
) -> Result<()> {
    // 1. 获取该broker下的所有partition
    let affected_partitions = self.partition_info.get_partitions_by_broker(failed_broker_id);
    
    // 2. 按partition_cluster_id分组
    let mut affected_clusters: HashMap<u32, Vec<PartitionInfo>> = HashMap::new();
    for partition in affected_partitions {
        affected_clusters
            .entry(partition.partition_cluster_id)
            .or_insert_with(Vec::new)
            .push(partition);
    }
    
    // 3. 对每个受影响的集群执行选举
    for (cluster_id, partitions) in affected_clusters {
        self.elect_new_leader_for_cluster(cluster_id, failed_broker_id, partitions).await?;
    }
    
    Ok(())
}
```

**步骤3：执行选举**
```rust
async fn elect_new_leader_for_cluster(
    &self,
    cluster_id: u32,
    failed_broker_id: u32,
    failed_partitions: Vec<PartitionInfo>,
) -> Result<()> {
    // 1. 获取该partition集群的完整信息
    let cluster_info = self.partition_info.partition_cluster
        .get(&cluster_id)
        .ok_or_else(|| anyhow!("Cluster {} not found", cluster_id))?;
    
    // 2. 检查失联的broker是否包含当前Leader
    let old_leader_id = cluster_info.lp_id;
    let is_leader_failed = failed_partitions.iter()
        .any(|p| p.id == old_leader_id && p.is_leader);
    
    if !is_leader_failed {
        // Leader未失联，只需移除失联的Follower
        self.remove_failed_followers(cluster_id, failed_partitions).await?;
        return Ok(());
    }
    
    // 3. Leader已失联，需要选举新的Leader
    // 3.1 获取该集群下所有存活的partition（排除失联broker的partition）
    let all_cluster_partitions = self.partition_info.get_partitions_by_cluster(cluster_id);
    let alive_partitions: Vec<PartitionInfo> = all_cluster_partitions.iter()
        .filter(|p| p.broker_id != failed_broker_id)
        .cloned()
        .collect();
    
    if alive_partitions.is_empty() {
        error!("No alive partitions in cluster {}, cannot elect new leader", cluster_id);
        return Err(anyhow!("No alive partitions"));
    }
    
    // 3.2 从存活的Follower中选择新的Leader（选择LEO最大的Follower）
    let new_leader = alive_partitions.iter()
        .max_by_key(|p| p.leo)
        .ok_or_else(|| anyhow!("Failed to select new leader"))?;
    
    // 3.3 更新PartitionClusterInfo
    let mut cluster_info_mut = cluster_info.clone();
    cluster_info_mut.lp_id = new_leader.id;
    cluster_info_mut.epoch += 1;  // 递增epoch
    cluster_info_mut.fp_ids = alive_partitions.iter()
        .filter(|p| p.id != new_leader.id)
        .map(|p| p.id)
        .collect();
    
    // 3.4 更新coo中的PartitionClusterInfo
    self.partition_info.partition_cluster.insert(cluster_id, cluster_info_mut.clone());
    
    // 3.5 更新新Leader的PartitionInfo
    let mut new_leader_info = new_leader.clone();
    new_leader_info.is_leader = true;
    new_leader_info.epoch = cluster_info.epoch + 1;
    self.partition_info.update_partition(new_leader_info);
    
    // 3.6 更新其他Follower的PartitionInfo（确保is_leader=false）
    for partition in alive_partitions.iter().filter(|p| p.id != new_leader.id) {
        let mut follower_info = partition.clone();
        follower_info.is_leader = false;
        follower_info.epoch = cluster_info.epoch + 1;
        self.partition_info.update_partition(follower_info);
    }
    
    // 3.7 通知所有存活的broker（通过心跳响应返回角色变更信息）
    self.notify_brokers_role_change(cluster_id, new_leader.id).await?;
    
    info!("Elected new leader {} for cluster {} (epoch: {})", 
          new_leader.id, cluster_id, cluster_info.epoch + 1);
    
    Ok(())
}
```

**步骤4：Broker处理角色变更**
```rust
async fn handle_role_change(
    &self,
    role_change: RoleChange,
) -> Result<()> {
    let partition = self.partitions.get(&role_change.partition_id)
        .ok_or_else(|| anyhow!("Partition not found"))?;
    
    // 1. 检查epoch，防止使用过期的角色信息
    if role_change.epoch < partition.epoch {
        warn!("Received stale role change, ignoring");
        return Ok(());
    }
    
    // 2. 更新本地角色
    partition.is_leader = role_change.is_leader;
    partition.epoch = role_change.epoch;
    
    if role_change.is_leader {
        // 3. 变为Leader：开始接受写入请求
        partition.enable_write().await?;
        partition.stop_sync().await?;
    } else {
        // 4. 变为Follower：停止写入，建立/复用与Leader的TCP同步连接
        partition.disable_write().await?;
        
        let cluster_info = self.clusters.get(&partition.partition_cluster_id)?;
        let leader_broker_id = self.get_leader_broker_id(cluster_info.lp_id).await?;
        
        // 5. 检查是否已有TCP连接，如果没有则新建，如果有则复用
        partition.ensure_replication_connection(leader_broker_id).await?;
    }
    
    Ok(())
}
```

**任务清单**:
1. [ ] 实现Broker超时检测逻辑
2. [ ] 实现受影响的Partition集群识别
3. [ ] 实现Leader选举逻辑
4. [ ] 实现角色变更通知机制
5. [ ] 实现Broker角色变更处理
6. [ ] 实现Epoch机制防止过期Leader
7. [ ] 编写单元测试和集成测试

**验收标准**:
- Broker超时能够被正确检测
- Leader失联时能够正确选举新Leader
- 选举后能够正确通知所有Broker
- Broker能够正确处理角色变更
- Epoch机制能够防止过期Leader写入

### 4.6 阶段6：Broker启动和恢复

**目标**: 实现Broker启动时的角色初始化和恢复逻辑

**实现流程**:
```rust
async fn broker_startup(broker: &Broker) -> Result<()> {
    // 1. 从Coordinator获取所有Partition信息
    let partition_info = coordinator.get_all_partitions(broker.id).await?;
    
    // 2. 对比本地和Coordinator的角色
    for coordinator_partition in partition_info {
        if let Some(local_partition) = broker.partitions.get(&coordinator_partition.id) {
            // 2.1 检查epoch，Coordinator的epoch是权威的
            if coordinator_partition.epoch > local_partition.epoch {
                // Coordinator的epoch更新，使用Coordinator的角色
                broker.update_partition_role(coordinator_partition).await?;
            } else if coordinator_partition.epoch < local_partition.epoch {
                // 本地epoch更新（不应该发生），报告错误
                error!("Local epoch {} is newer than coordinator epoch {}",
                       local_partition.epoch, coordinator_partition.epoch);
                // 使用Coordinator的角色（降级）
                broker.update_partition_role(coordinator_partition).await?;
            } else {
                // epoch相同，使用Coordinator的角色（权威）
                broker.update_partition_role(coordinator_partition).await?;
            }
        } else {
            // Coordinator中没有此Partition信息
            // 可能是新创建的，或者Coordinator数据丢失
            // 保守策略：等待Coordinator分配
            warn!("Partition {} not found in coordinator, waiting for assignment",
                  coordinator_partition.id);
        }
    }
    
    // 3. 根据角色初始化
    for partition in broker.partitions.iter() {
        if partition.is_leader {
            partition.enable_write().await?;
        } else {
            partition.disable_write().await?;
            partition.ensure_replication_connection().await?;
        }
    }
    
    // 4. 启动心跳循环
    broker.start_heartbeat_loop().await?;
    
    Ok(())
}
```

**任务清单**:
1. [ ] 实现Broker启动时的角色初始化
2. [ ] 实现Epoch验证逻辑
3. [ ] 实现角色冲突处理
4. [ ] 编写单元测试和集成测试

**验收标准**:
- Broker启动时能够从Coordinator获取正确的角色信息
- Epoch验证能够防止双主问题
- 角色冲突能够被正确处理

---

## 5. 故障处理和最佳实践

### 5.1 Broker与Coordinator失联处理

**场景1：Leader与Coordinator失联**
- **行为**: 租约过期后停止写入，允许读取已提交数据（HW之前的数据）
- **实现**: 
  ```rust
  async fn leader_handle_coordinator_disconnect(&self) {
      // 1. 记录失联时间
      self.coordinator_disconnect_time = Some(current_timestamp());
      
      // 2. 继续服务读取请求（仅读取HW之前的数据）
      // 3. 拒绝写入请求（避免双主）
  }
  
  async fn leader_check_lease(&self) -> bool {
      if let Some(disconnect_time) = self.coordinator_disconnect_time {
          let elapsed = current_timestamp() - disconnect_time;
          if elapsed > LEASE_DURATION {
              // 租约过期，停止写入
              return false;
          }
      }
      true
  }
  ```

**场景2：Follower与Coordinator失联**
- **行为**: 不直接服务客户端请求，继续从Leader同步数据（如果连接正常）
- **实现**: Follower本身就不服务客户端请求，只需继续同步即可

**场景3：客户端处理**
- **行为**: 
  - 定期刷新元数据（如每30秒）
  - 写入失败时立即刷新并重试
  - 自动故障转移到可用的Leader
- **实现**:
  ```rust
  impl Client {
      async fn produce(&mut self, topic: &str, msg: Message) -> Result<()> {
          let mut retries = 3;
          loop {
              match self.try_produce(topic, &msg).await {
                  Ok(offset) => return Ok(offset),
                  Err(Error::NotLeader) | Err(Error::LeaderChanged) => {
                      // 立即刷新元数据
                      self.refresh_metadata().await?;
                      retries -= 1;
                      if retries == 0 {
                          return Err(Error::MaxRetriesExceeded);
                      }
                  }
                  Err(e) => return Err(e),
              }
          }
      }
  }
  ```

### 5.2 双主问题防护

**机制1：Epoch机制**
- 每次选举时递增epoch
- Leader写入时携带epoch
- 过期Leader的写入请求会被拒绝

**机制2：Broker启动时角色验证**
- Broker启动时从Coordinator获取角色信息
- 对比本地和Coordinator的epoch
- 使用Coordinator的角色（权威）

**机制3：租约机制**
- Leader定期续租（通过心跳）
- 租约过期后自动降级为Follower
- 恢复的旧Leader发现租约过期，自动降级

### 5.3 TCP连接优化

**问题**: 多个Broker之间因为Partition集群，可能存在很多TCP连接

**解决方案：连接池/多路复用**
- 同一Broker对之间的所有Partition共享一个TCP连接
- 连接内使用stream_id区分不同Partition的数据流
- 连接数从O(N × M)降至O(N²)

**实现**:
```rust
struct BrokerConnectionPool {
    // target_broker_id => Connection
    connections: DashMap<u32, Arc<ReplicationConnection>>,
}

struct ReplicationConnection {
    stream_id_generator: AtomicU32,
    // stream_id => PartitionReplicationStream
    streams: DashMap<u32, PartitionReplicationStream>,
}
```

---

## 6. 测试方案

### 6.1 单元测试

**数据结构测试**:
- [ ] 测试索引的增删改查
- [ ] 测试并发安全性
- [ ] 测试索引一致性

**ID生成测试**:
- [ ] 测试Partition ID的唯一性和递增性
- [ ] 测试Cluster ID的唯一性和可预测性
- [ ] 测试ID生成的持久化和恢复

**HW/LEO计算测试**:
- [ ] 测试HW计算逻辑（min(所有ISR的LEO)）
- [ ] 测试LEO更新逻辑
- [ ] 测试HW更新时机

**选举逻辑测试**:
- [ ] 测试Leader失联时的选举
- [ ] 测试Follower失联时的处理
- [ ] 测试Epoch机制
- [ ] 测试选举后的角色更新

### 6.2 集成测试

**场景1：正常数据同步**
- [ ] 创建Topic和Partition
- [ ] 写入消息到Leader
- [ ] 验证Follower能够同步数据
- [ ] 验证HW正确更新
- [ ] 验证消费者能够读取到数据

**场景2：Leader故障转移**
- [ ] 创建Topic和Partition（3个副本）
- [ ] 写入消息到Leader
- [ ] 停止Leader Broker
- [ ] 验证新Leader被选举
- [ ] 验证数据不丢失
- [ ] 验证消费者能够继续消费

**场景3：Broker恢复**
- [ ] 创建Topic和Partition
- [ ] 停止一个Broker
- [ ] 等待选举完成
- [ ] 恢复Broker
- [ ] 验证Broker能够正确恢复角色
- [ ] 验证不会出现双主问题

**场景4：网络分区**
- [ ] 创建Topic和Partition（3个副本）
- [ ] 模拟网络分区（Leader与部分Follower隔离）
- [ ] 验证系统行为（写入停止或只读模式）
- [ ] 恢复网络
- [ ] 验证数据一致性

**场景5：并发写入和同步**
- [ ] 多个Producer并发写入
- [ ] 验证数据一致性
- [ ] 验证HW正确更新
- [ ] 验证性能指标

### 6.3 压力测试

**测试指标**:
- [ ] 吞吐量：消息写入TPS
- [ ] 延迟：消息同步延迟
- [ ] 故障恢复时间：Leader选举时间
- [ ] 资源使用：CPU、内存、网络

**测试场景**:
- [ ] 大量Partition的创建和分配
- [ ] 高频心跳和元数据更新
- [ ] 大量并发数据同步
- [ ] 频繁的Leader选举

### 6.4 混沌测试

**测试场景**:
- [ ] 随机停止Broker
- [ ] 随机网络延迟
- [ ] 随机网络丢包
- [ ] 随机消息丢失
- [ ] 验证系统能够自动恢复

---

## 7. 实施时间表

### 阶段1：基础数据结构（1周）
- 数据结构设计和实现
- 索引管理实现
- 单元测试

### 阶段2：Topic创建和分配（1周）
- Partition分配逻辑
- ID生成机制
- 集成测试

### 阶段3：心跳和元数据同步（1周）
- 心跳机制实现
- 元数据同步
- 角色变更通知

### 阶段4：数据同步机制（2周）
- Follower拉取逻辑
- Leader处理逻辑
- HW计算和更新
- TCP连接多路复用

### 阶段5：Leader选举（2周）
- 超时检测
- 选举逻辑
- 角色变更处理
- Epoch机制

### 阶段6：Broker启动和恢复（1周）
- 启动时角色初始化
- 恢复逻辑
- 双主防护

### 阶段7：测试和优化（2周）
- 单元测试
- 集成测试
- 压力测试
- 性能优化

**总计：约10周**

---

## 8. 注意事项和风险

### 8.1 数据一致性保证
- **HW更新时机**: Follower收到数据后先更新HW，落盘成功后才更新LEO
- **写入确认**: 只有HW之前的数据才能被消费者读取
- **Epoch机制**: 防止过期Leader写入，保证数据一致性

### 8.2 性能考虑
- **同步 vs 异步**: 当前设计为异步同步，保证写入性能
- **批量传输**: 支持批量拉取数据，减少网络开销
- **连接复用**: TCP连接多路复用，减少连接数

### 8.3 故障恢复
- **选举时间**: Leader选举应在秒级完成
- **数据追赶**: Follower需要能够快速追赶Leader
- **网络分区**: 需要明确处理策略（一致性优先或可用性优先）

### 8.4 监控和告警
- **关键指标**: 
  - Broker心跳延迟
  - Partition同步延迟
  - HW更新频率
  - Leader选举次数
- **告警规则**:
  - Broker心跳超时
  - Partition同步延迟过大
  - Leader频繁选举

---

## 9. 参考资料

- Kafka副本机制设计
- Raft一致性算法
- 分布式系统一致性理论

---

## 10. 附录：常见问题解答

### Q1: 为什么Follower和Leader使用相同的partition_id？
**A**: 更符合Kafka的设计理念（Partition ID在Topic内唯一），简化了数据结构和查询逻辑。通过`(broker_id, partition_cluster_id)`可以唯一标识任何Partition副本。

### Q2: 如何避免双主问题？
**A**: 通过Epoch机制、Broker启动时角色验证、租约机制三重防护，确保不会出现双主。

### Q3: Broker与Coordinator失联时应该如何处理？
**A**: Leader失联时租约过期后停止写入，允许读取已提交数据；Follower失联时不直接服务客户端请求；客户端应刷新元数据并切换到可用的Leader。

### Q4: TCP连接多路复用如何实现？
**A**: 同一Broker对之间的所有Partition共享一个TCP连接，连接内使用stream_id区分不同Partition的数据流。

### Q5: 是否可以使用QUIC协议替换TCP？
**A**: 可以，QUIC具有多路复用、连接迁移等优势，但需要考虑实现复杂度和防火墙兼容性。建议先实现TCP版本，后续可考虑QUIC作为可选项。
